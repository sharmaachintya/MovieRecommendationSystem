{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MovieRecommendationSystem.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9e4sxsEIxB4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "#from spellchecker import SpellChecker # pyspellchecker\n",
        "\n",
        "import re, os, math, sklearn, datetime, pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movies_df = pd.read_csv('movies.csv')\n",
        "ratings_df = pd.read_csv('ratings.csv').iloc[:500000, :]  ## Change the \"500000\" to your desired size\n",
        "tags_df = pd.read_csv('tags.csv')"
      ],
      "metadata": {
        "id": "EBZDfUucJDDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t8vr1kik3ZMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a04380c-c774-4910-e06e-0ac0ea25a7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movies_df.info()"
      ],
      "metadata": {
        "id": "6DXfRj5tYHYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e95b96-6f3e-4df7-e5ca-a0e01de1a2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 62423 entries, 0 to 62422\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   movieId  62423 non-null  int64 \n",
            " 1   title    62423 non-null  object\n",
            " 2   genres   62423 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulQbGYQeNXq8",
        "outputId": "ecff5d9c-4a49-4437-da86-5ea9a04a6049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 0 to 499999\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count   Dtype  \n",
            "---  ------     --------------   -----  \n",
            " 0   userId     500000 non-null  int64  \n",
            " 1   movieId    500000 non-null  int64  \n",
            " 2   rating     500000 non-null  float64\n",
            " 3   timestamp  500000 non-null  int64  \n",
            "dtypes: float64(1), int64(3)\n",
            "memory usage: 15.3 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02qPlD06NXyF",
        "outputId": "5610d985-1051-4d12-e711-b18cf1320c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1093360 entries, 0 to 1093359\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count    Dtype \n",
            "---  ------     --------------    ----- \n",
            " 0   userId     1093360 non-null  int64 \n",
            " 1   movieId    1093360 non-null  int64 \n",
            " 2   tag        1093344 non-null  object\n",
            " 3   timestamp  1093360 non-null  int64 \n",
            "dtypes: int64(3), object(1)\n",
            "memory usage: 33.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving the data out of ratings_df and tags_df for the last movie the user liked to be used as the label:\n",
        "if os.path.exists('data/') != True:\n",
        "    os.mkdir('data/')\n",
        "    \n",
        "if os.path.exists('data/last_liked_tags/') != True: \n",
        "    os.mkdir('data/last_liked_tags/')\n",
        "    \n",
        "# Getting the last movie liked from ratings_df:\n",
        "ratings_df_copy = ratings_df.copy()\n",
        "tags_df_copy = tags_df.copy()\n",
        "\n",
        "users_list = list(set(ratings_df_copy.userId)) ## List of all users in the dataset\n",
        "\n",
        "ratings_index_list = [] ## These empty lists will be used to remove the last liked movies from the ratings_df and tags_df_mod copies\n",
        "tags_index_list = []\n",
        "\n",
        "last_ratings_df = pd.DataFrame() ## Want to save all the last liked movies rated into a single CSV file\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for user in users_list:\n",
        "    try: ## Some users did not rate a movie highly enough and will be removed from the dataset\n",
        "        temp_df = ratings_df_copy[ratings_df_copy.userId == user].copy()\n",
        "        temp_df = temp_df[temp_df.rating >= 4]\n",
        "\n",
        "        last_time = max(temp_df.timestamp) ## If the user did not have a \"liked\" movie, this will return an error\n",
        "\n",
        "        temp_df = temp_df[temp_df.timestamp == last_time] ## Isolating the last liked movie rated for each user\n",
        "        \n",
        "        if len(temp_df) > 1: ## Some of the movies were rated at the same timestamp; only the last one on spliced DF will be removed\n",
        "            temp_df = temp_df.iloc[[len(temp_df)-1]]\n",
        "            \n",
        "        ratings_index_list.append(temp_df.index.values[0]) ## Appending the index of the last movies watched\n",
        "\n",
        "        if counter == 0:\n",
        "            last_ratings_df = temp_df\n",
        "            counter = 1\n",
        "        else:\n",
        "            last_ratings_df = pd.concat([last_ratings_df, temp_df], ignore_index= True)\n",
        "        \n",
        "    except Exception:\n",
        "        ratings_index_list.append(ratings_df_copy[ratings_df_copy.userId == user].index.values[0]) ## Adding the index of the users whom did not highly rate a movie\n",
        "    \n",
        "    try:  ## Some users have not created tags\n",
        "        temp_df = tags_df_copy[tags_df_copy.userId == user].copy()\n",
        "        temp_df = temp_df[temp_df.rating >= 4]\n",
        "        last_movie = temp_df.movieId.values[0]\n",
        "        temp_df = temp_df[temp_df.movieId == last_movie]\n",
        "        \n",
        "        \n",
        "        if len(temp_df) == 0: ## MOST USERS DID NOT CREATE TAG(S) FOR THE LAST MOVIE LIKED\n",
        "            continue\n",
        "            \n",
        "        else:\n",
        "            temp_df.to_csv('data/last_liked_tags/' + str(user) + '.csv', index = False) \n",
        "            tags_index_list.extend(list(temp_df.index.values))  ## This is a .extend since there are most likely more than one timestamp per movie\n",
        "    \n",
        "    except Exception:\n",
        "        pass\n",
        "    \n",
        "last_ratings_df.to_csv('data/last_liked_ratings.csv', index = False)\n",
        "\n",
        "# Removing the last movies from ratings_df_copy and tags_df_copy:\n",
        "ratings_df_removed = ratings_df_copy.drop(ratings_index_list)\n",
        "\n",
        "tags_df_removed = tags_df_copy.drop(tags_index_list)\n",
        "\n",
        "ratings_df_removed.to_csv('data/ratings_df_last_liked_movie_removed.csv', index = False)\n",
        "tags_df_removed.to_csv('data/tags_df_last_liked_movie_removed.csv', index = False)\n"
      ],
      "metadata": {
        "id": "krTraErvNe8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Deleting 'timestamp' column since it is not informative (not examining viewer's social behaviors)\n",
        "## Droping all NaN values, which is only seen in the tag column--this is to eliminate considering NaNs when looping below\n",
        "tags_df_removed = pd.read_csv('data/tags_df_last_liked_movie_removed.csv')\n",
        "\n",
        "tags_df_mod = tags_df_removed.copy().drop('timestamp', axis=1).dropna()\n",
        "tags_df_mod['tag'] = tags_df_mod['tag'].str.lower()# Making all tags lowercased for uniform format\n",
        "\n",
        "\n",
        "for index, row in tags_df_mod.iterrows():\n",
        "    tag = row.tag#.split()  ## splitting words for spell check\n",
        "    \n",
        "    correct_tag = re.sub(r' \\([^)]*\\)', '', tag)  ## Removing all parenthesis and its contents, including the whitespace before\n",
        "        \n",
        "    # First if:\n",
        "    if 'based' in correct_tag: ## This is necessary because it is a common tag and avoids the other if statements downstream\n",
        "        tags_df_mod.loc[index, 'tag'] = correct_tag\n",
        "        continue\n",
        "        \n",
        "    # Second if:    \n",
        "    if '-' in correct_tag: ## This is to keep \"sci-fi\" from being removed in the next if statement\n",
        "        tags_df_mod.loc[index, 'tag'] = correct_tag\n",
        "        continue\n",
        "        \n",
        "    # Third if:    \n",
        "    if re.findall(r'\\b\\w{2}\\b', correct_tag):\n",
        "        tags_df_mod.loc[index, 'tag'] = np.NaN ## Replacing two-letter words; Need to maintain index ordering, will delete NaNs later\n",
        "        \n",
        "    elif re.findall(r'\\b\\w{1}\\b', correct_tag):\n",
        "        tags_df_mod.loc[index, 'tag'] = np.NaN ## Replacing one-letter words\n",
        "        \n",
        "    elif tag == correct_tag: ## This is for better performance since replacing significantly slows the process\n",
        "        continue\n",
        "        \n",
        "    else:\n",
        "        tags_df_mod.loc[index, 'tag'] = correct_tag ## Saves the corrected tag\n",
        "        pass\n",
        "        \n",
        "tags_df_mod = tags_df_mod.dropna() # Dropping all tags with words that are lower than two letters or less\n",
        "\n",
        "tags_df_mod.to_csv('data/tags_df_mod.csv', index = False)"
      ],
      "metadata": {
        "id": "ds9fPqymGgAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new DF that contains the most common tags for each movie (\"movieId\"):\n",
        "\n",
        "## This will create a new DF for each movie and will store this file since there is no easy storage method for this task\n",
        "### Storage will be in the \"data\" folder under the \"movie_tags\" subfolder:\n",
        "    \n",
        "if os.path.exists('data/movie_tags/') != True: # Creating movie_tags subfolder\n",
        "    os.mkdir('data/movie_tags/')\n",
        "    \n",
        "## Creating a copy of tags_df_mod and dropping userID:\n",
        "tags_df_mod = pd.read_csv('data/tags_df_mod.csv')\n",
        "\n",
        "tags_df_no_user = tags_df_mod.copy().drop('userId', axis= 1)\n",
        "\n",
        "## Obtaining a list of all movieId with tags:\n",
        "### !!!! The set() function does not put the list in perfect order. Some of the IDs are out-of-place.\n",
        "movieId_list = list(set(tags_df_no_user.movieId))  \n",
        "\n",
        "for movieId in movieId_list:\n",
        "    df_select = tags_df_no_user[tags_df_no_user.movieId == movieId].copy().drop('movieId', axis= 1)\n",
        "    \n",
        "    df_select['COUNT'] = 1\n",
        "    \n",
        "    df_select_group = df_select.groupby(['tag']).count()\n",
        "    \n",
        "    df_select_group = df_select_group.sort_values(by=['COUNT'], ascending= False).reset_index()\n",
        "    \n",
        "    df_select_group.to_csv('data/movie_tags/' + str(movieId) + '.csv', index = False)"
      ],
      "metadata": {
        "id": "s0joKimf3HSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new DF that contains the most common tags for each user (\"userId\"):\n",
        "## This DF is similar to the movieId DF that is previously created except this ties the tags in with each user\n",
        "### This can be used in conjunction with the most common genres watched by the user to help determine which movies they like to watch\n",
        "if os.path.exists('data/user_tags/') != True: # Creating movie_tags subfolder\n",
        "    os.mkdir('data/user_tags/')\n",
        "    \n",
        "## Creating a copy of tags_df_mod and dropping userID:\n",
        "tags_df_mod = pd.read_csv('data/tags_df_mod.csv')\n",
        "\n",
        "tags_df_user = tags_df_mod.copy().drop('movieId', axis= 1)\n",
        "\n",
        "## Obtaining a list of all movieId with tags:\n",
        "userId_list = list(set(tags_df_user.userId))\n",
        "\n",
        "for userId in userId_list:\n",
        "    df_select = tags_df_user[tags_df_user.userId == userId].copy().drop('userId', axis= 1)\n",
        "    \n",
        "    df_select['COUNT'] = 1\n",
        "    \n",
        "    df_select_group = df_select.groupby(['tag']).count()\n",
        "    \n",
        "    df_select_group = df_select_group.sort_values(by=['COUNT'], ascending= False).reset_index()\n",
        "    \n",
        "    df_select_group.to_csv('data/user_tags/' + str(userId) + '.csv', index = False)"
      ],
      "metadata": {
        "id": "v0mnGNuQ3bnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating another DF that contains the most common tags created by users:\n",
        "## Common = the tag was used 35 times or more\n",
        "tags_df_mod = pd.read_csv('data/tags_df_mod.csv')\n",
        "\n",
        "common_tags_df = tags_df_mod.groupby(['tag']).count().sort_values('userId', ascending= False).copy().drop('movieId', axis= 1).reset_index()\n",
        "\n",
        "common_tags_df = common_tags_df[common_tags_df.userId >= 35]\n",
        "\n",
        "common_tags_df.to_csv('data/common_tags.csv', index = False)"
      ],
      "metadata": {
        "id": "61Fvd21A3d-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df_removed = pd.read_csv('data/ratings_df_last_liked_movie_removed.csv')\n",
        "movies_df_mod = movies_df.copy()\n",
        "\n",
        "movies_df_mod['YEAR'] = 0\n",
        "movies_df_mod['UPPER_STD'] = 0\n",
        "movies_df_mod['LOWER_STD'] = 0\n",
        "movies_df_mod['AVG_RATING'] = 0\n",
        "movies_df_mod['VIEW_COUNT'] = 0\n",
        "\n",
        "# Making the genres into columns:\n",
        "## First, need to obtain a list of all the genres in the dataset.\n",
        "#### !!!! Note: \"IMAX\" is not listed in the readme but is present in the dataset. \"Children's\" in the readme is \"Children\" in the dataset.\n",
        "genres_list = []\n",
        "for index, row in movies_df.iterrows():\n",
        "    try:\n",
        "        genres = row.genres.split('|')\n",
        "        genres_list.extend(genres)\n",
        "    except:\n",
        "        genres_list.append(row.genres)\n",
        "        \n",
        "genres_list = list(set(genres_list))\n",
        "genres_list.remove('IMAX')\n",
        "genres_list.remove('(no genres listed)') # Replace with 'None'\n",
        "genres_list.append('None')\n",
        "\n",
        "for genre in genres_list: # Creating new columns with names as genres\n",
        "    movies_df_mod[genre] = 0  # 0 = movie is not considered in that genre\n",
        "\n",
        "\n",
        "for index, row in movies_df_mod.iterrows():\n",
        "    movieId = row.movieId\n",
        "    title = row.title\n",
        "    \n",
        "    try:\n",
        "        genres = row.genres.split('|') ## Multiple genres for the movie is separated by '|' in the one string; converts to list\n",
        "    except Exception:\n",
        "        genres = list(row.genres) ## In the case that there is only one genre for the movie\n",
        "        \n",
        "        \n",
        "    #print(index)\n",
        "    \n",
        "    # Extracting the year from the title:\n",
        "    try: ## Some titles do not have the year\n",
        "        matcher = re.compile('\\(\\d{4}\\)')  ## Need to extract '(year)' from the title in case there is a year in the title\n",
        "        parenthesis_year = matcher.search(title).group(0)\n",
        "        matcher = re.compile('\\d{4}') ## Matching the year from the already matched '(year)'\n",
        "        year = matcher.search(parenthesis_year).group(0)\n",
        "\n",
        "        movies_df_mod.loc[index, 'YEAR'] = int(year)\n",
        "    \n",
        "    except Exception:\n",
        "        pass\n",
        "    \n",
        "    # Merging info from ratings_df into movies_df\n",
        "    try:\n",
        "        ratings_df_select = ratings_df_removed[ratings_df_removed.movieId == movieId]  ## Gathering the reviews for the movies\n",
        "        std = np.std(ratings_df_select.rating)\n",
        "        average_rating = np.mean(ratings_df_select.rating)\n",
        "\n",
        "        upper_std = average_rating + std\n",
        "\n",
        "        if upper_std > 5:   # This is to prevent the upper range from passing the max rating value\n",
        "            upper_std = 5\n",
        "\n",
        "        lower_std = average_rating - std\n",
        "\n",
        "        if lower_std < 0.5:\n",
        "            lower_std = 0.5\n",
        "\n",
        "        view_count = len(ratings_df_select)\n",
        "\n",
        "        movies_df_mod.loc[index, 'UPPER_STD'] = upper_std\n",
        "        movies_df_mod.loc[index, 'LOWER_STD'] = lower_std\n",
        "        movies_df_mod.loc[index, 'AVG_RATING'] = average_rating\n",
        "        movies_df_mod.loc[index, 'VIEW_COUNT'] = view_count\n",
        "        \n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    \n",
        "    # Changing all columns that are labelled as genres to 1 if the movie is in that genre:\n",
        "    if 'IMAX' in genres:\n",
        "        genres.remove('IMAX')\n",
        "        \n",
        "    if '(no genres listed)' in genres:\n",
        "        genres.remove('(no genres listed)')\n",
        "        genres.append('None')\n",
        "        \n",
        "    for genre in genres:\n",
        "        movies_df_mod.loc[index, genre] = 1\n",
        "        \n",
        "movies_df_mod = movies_df_mod[movies_df_mod.YEAR != 0] ## Removing all movies without years in the title\n",
        "movies_df_mod = movies_df_mod[movies_df_mod.VIEW_COUNT != 0] ## Removing all movies than have not be rated\n",
        "\n",
        "movies_df_mod.to_csv('data/movies_mod.csv', index = False)"
      ],
      "metadata": {
        "id": "YxyEnnuT3gPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining ratings_df and movies_df_mod together:\n",
        "movies_df_mod = pd.read_csv('data/movies_mod.csv')\n",
        "\n",
        "ratings_df_removed = pd.read_csv('data/ratings_df_last_liked_movie_removed.csv')\n",
        "\n",
        "ratings_movies_df = ratings_df_removed.merge(movies_df_mod, how= 'left', on= 'movieId').dropna()  ## Some of the movies were removed when creating movies_df_mod, which will result in nan values for some rows\n",
        "\n",
        "# Getting a count of all the liked and dislike genres and transforming it into a percentage (liked genre counts / all liked genres counts)\n",
        "## If the user rated the movie 4+, then they liked it. If lower than 4, then they disliked it.\n",
        "users_list = list(set(ratings_movies_df.userId))\n",
        "\n",
        "total_user_like_df = pd.DataFrame()\n",
        "total_user_dislike_df = pd.DataFrame()\n",
        "\n",
        "progress_counter_1 = 0\n",
        "progress_counter_2 = .10\n",
        "\n",
        "for user in users_list:\n",
        "    temp_df = ratings_movies_df[ratings_movies_df.userId == user]\n",
        "    like_df = temp_df[temp_df.rating >= 4].iloc[:, 14:] ## Only selecting the genres\n",
        "    dislike_df = temp_df[temp_df.rating < 4].iloc[:, 14:]\n",
        "    \n",
        "    liked_total_counts = 0\n",
        "    liked_dict = {'userId': user,'War': 0, 'Animation': 0, 'Horror': 0, 'Sci-Fi': 0, 'Fantasy': 0, 'Thriller': 0, 'Crime': 0, 'Mystery': 0, \n",
        "                  'Documentary': 0, 'Children': 0, 'Action': 0, 'Adventure': 0, 'Musical': 0,'Film-Noir': 0, 'Drama': 0, \n",
        "                  'Romance': 0, 'Comedy': 0, 'Western': 0, 'None': 0}\n",
        "    \n",
        "    disliked_total_counts = 0\n",
        "    disliked_dict = {'userId': user,'War': 0, 'Animation': 0, 'Horror': 0, 'Sci-Fi': 0, 'Fantasy': 0, 'Thriller': 0, 'Crime': 0, 'Mystery': 0, \n",
        "                  'Documentary': 0, 'Children': 0, 'Action': 0, 'Adventure': 0, 'Musical': 0,'Film-Noir': 0, 'Drama': 0, \n",
        "                  'Romance': 0, 'Comedy': 0, 'Western': 0, 'None': 0}   \n",
        "    \n",
        "    progress_counter_1 += 1\n",
        "    if progress_counter_1 / len(users_list) >= progress_counter_2:\n",
        "        print(progress_counter_1 / len(users_list) * 100, '%')\n",
        "        progress_counter_2 += .10\n",
        "    \n",
        "    for genre in list(like_df.columns): ## Getting all the genre counts for liked and disliked, separately\n",
        "        if len(like_df) == 0: ## If the user has not given a movie a rating of 4 or higher\n",
        "            pass\n",
        "        \n",
        "        else:\n",
        "            liked_total_counts += sum(like_df[genre])\n",
        "        \n",
        "        \n",
        "        if len(dislike_df) == 0: ## If the user has not given a movie a rating of 3.5 or lower\n",
        "            pass\n",
        "        \n",
        "        else:\n",
        "            disliked_total_counts += sum(dislike_df[genre])\n",
        "        \n",
        "        \n",
        "    for genre in list(like_df.columns):\n",
        "        if liked_total_counts == 0: \n",
        "            pass\n",
        "        \n",
        "        else:\n",
        "            liked_genre_total_counts = sum(like_df[genre])\n",
        "            liked_dict[genre] = liked_genre_total_counts/liked_total_counts\n",
        "            \n",
        "            \n",
        "        if disliked_total_counts == 0:\n",
        "            pass\n",
        "        \n",
        "        else:\n",
        "            disliked_genre_total_counts = sum(dislike_df[genre])\n",
        "            disliked_dict[genre] = disliked_genre_total_counts/disliked_total_counts\n",
        "        \n",
        "    \n",
        "    user_like_df = pd.DataFrame(liked_dict, index=[0]) ## Even though some users have not rated a movie higher or lower than 4, the zero counts will still be added for complete-ness\n",
        "    user_dislike_df = pd.DataFrame(disliked_dict, index=[0])\n",
        "    \n",
        "    # Concatenating the user total counts \n",
        "    if len(total_user_like_df) == 0:\n",
        "        total_user_like_df = user_like_df\n",
        "    \n",
        "    else:\n",
        "        total_user_like_df = pd.concat([total_user_like_df, user_like_df], ignore_index= True)\n",
        "        \n",
        "    if len(total_user_dislike_df) == 0:\n",
        "        total_user_dislike_df = user_dislike_df\n",
        "        \n",
        "    else:\n",
        "        total_user_dislike_df = pd.concat([total_user_dislike_df, user_dislike_df], ignore_index= True)\n",
        "        \n",
        "total_user_like_df.to_csv('data/total_user_like_df.csv', index = False)\n",
        "total_user_dislike_df.to_csv('data/total_user_dislike_df.csv', index = False)"
      ],
      "metadata": {
        "id": "iw9hEExrRbJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf592b5-508e-4c78-8ca3-48f8e27d86fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.014513788098693 %\n",
            "20.0 %\n",
            "30.014513788098697 %\n",
            "40.0 %\n",
            "50.01451378809869 %\n",
            "60.0 %\n",
            "70.0145137880987 %\n",
            "80.0 %\n",
            "90.0145137880987 %\n",
            "100.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary of vectorized tags:\n",
        "if os.path.exists('data/final/') != True:\n",
        "    os.mkdir('data/final/')"
      ],
      "metadata": {
        "id": "-BZa5uDPR-o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a movie tags profile to complement the user tags:\n",
        "if os.path.exists('data/final/') != True:\n",
        "    os.mkdir('data/final/')\n",
        "    \n",
        "movies_df_mod = pd.read_csv('data/movies_mod.csv')\n",
        "movieId_list = list(movies_df_mod.movieId)\n",
        "del movies_df_mod\n",
        "\n",
        "movie_tags_df = pd.DataFrame()\n",
        "index_counter = 0\n",
        "\n",
        "progress_counter_1 = 0\n",
        "progress_counter_2 = 5\n",
        "start_time = datetime.datetime.now()\n",
        "print('Start Time:', start_time)\n",
        "\n",
        "with open('data/vectorized_dict.pkl', 'rb') as reader:\n",
        "    vectorized_dict = pickle.load(reader)\n",
        "\n",
        "for movie in movieId_list:\n",
        "    progress_counter_1 += 1\n",
        "\n",
        "    try:\n",
        "        temp_df = pd.read_csv('data/movie_tags/{}.csv'.format(movie))  ## The tags are already in order of most counts and then alphabetically\n",
        "\n",
        "        if len(temp_df) < 5: ## Skipping movies with less than 5 tags\n",
        "            continue \n",
        "\n",
        "        vectorized_tag = []\n",
        "        movie_tags = list(temp_df.tag)\n",
        "\n",
        "        for tag in movie_tags:\n",
        "            try:\n",
        "                tag_vector = vectorized_dict[tag]\n",
        "                vectorized_tag.append(tag_vector)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        if len(vectorized_tag) < 5: ## Skipping movies with less than 5 common tags; The first similar if statement is not needed but is placed for performance purposes\n",
        "            continue \n",
        "\n",
        "        movie_tags_df.loc[index_counter, 'movieId'] = movie\n",
        "\n",
        "        for x in range(5):\n",
        "            movie_tags_df.loc[index_counter, 'TAG_' + str(x)] = vectorized_tag[x]\n",
        "            \n",
        "        index_counter += 1\n",
        "            \n",
        "    except Exception:\n",
        "        pass\n",
        "    \n",
        "    if (progress_counter_1 / len(movieId_list)) * 100 >= progress_counter_2:\n",
        "        print((progress_counter_1 / len(movieId_list)) * 100, '% completed')\n",
        "        print('Processing Time:', datetime.datetime.now() - start_time)\n",
        "        print('Current Time:', datetime.datetime.now())\n",
        "        progress_counter_2 += 5\n",
        "\n",
        "movie_tags_df.to_csv('data/final/movie_tags_df.csv', index = False)"
      ],
      "metadata": {
        "id": "pwbbfzHlSBqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "353f0a27-480f-4672-a5b7-db04bee97fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Time: 2022-05-14 17:23:17.921755\n",
            "5.003162919086779 % completed\n",
            "Processing Time: 0:00:01.738002\n",
            "Current Time: 2022-05-14 17:23:19.659828\n",
            "10.000575076197595 % completed\n",
            "Processing Time: 0:00:03.518388\n",
            "Current Time: 2022-05-14 17:23:21.440217\n",
            "15.003737995284375 % completed\n",
            "Processing Time: 0:00:05.332148\n",
            "Current Time: 2022-05-14 17:23:23.254294\n",
            "20.00115015239519 % completed\n",
            "Processing Time: 0:00:07.160251\n",
            "Current Time: 2022-05-14 17:23:25.082759\n",
            "25.00431307148197 % completed\n",
            "Processing Time: 0:00:08.906395\n",
            "Current Time: 2022-05-14 17:23:26.828240\n",
            "30.013226752544714 % completed\n",
            "Processing Time: 0:00:10.683061\n",
            "Current Time: 2022-05-14 17:23:28.604914\n",
            "35.004888147679566 % completed\n",
            "Processing Time: 0:00:12.454881\n",
            "Current Time: 2022-05-14 17:23:30.376701\n",
            "40.00230030479038 % completed\n",
            "Processing Time: 0:00:14.318933\n",
            "Current Time: 2022-05-14 17:23:32.241077\n",
            "45.005463223877165 % completed\n",
            "Processing Time: 0:00:16.137609\n",
            "Current Time: 2022-05-14 17:23:34.059473\n",
            "50.00287538098797 % completed\n",
            "Processing Time: 0:00:18.003814\n",
            "Current Time: 2022-05-14 17:23:35.925658\n",
            "55.006038300074756 % completed\n",
            "Processing Time: 0:00:19.989234\n",
            "Current Time: 2022-05-14 17:23:37.911057\n",
            "60.00345045718558 % completed\n",
            "Processing Time: 0:00:22.083085\n",
            "Current Time: 2022-05-14 17:23:40.005239\n",
            "65.0008626142964 % completed\n",
            "Processing Time: 0:00:23.950940\n",
            "Current Time: 2022-05-14 17:23:41.873205\n",
            "70.00977629535913 % completed\n",
            "Processing Time: 0:00:25.821074\n",
            "Current Time: 2022-05-14 17:23:43.742898\n",
            "75.00143769049399 % completed\n",
            "Processing Time: 0:00:27.742563\n",
            "Current Time: 2022-05-14 17:23:45.664386\n",
            "80.00460060958076 % completed\n",
            "Processing Time: 0:00:29.416419\n",
            "Current Time: 2022-05-14 17:23:47.338615\n",
            "85.00201276669158 % completed\n",
            "Processing Time: 0:00:30.930914\n",
            "Current Time: 2022-05-14 17:23:48.853093\n",
            "90.00517568577835 % completed\n",
            "Processing Time: 0:00:32.432560\n",
            "Current Time: 2022-05-14 17:23:50.354768\n",
            "95.00258784288918 % completed\n",
            "Processing Time: 0:00:33.934614\n",
            "Current Time: 2022-05-14 17:23:51.856775\n",
            "100.0 % completed\n",
            "Processing Time: 0:00:35.381695\n",
            "Current Time: 2022-05-14 17:23:53.303842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stats(predictions, true, flex_range= 0.5):\n",
        "    predictions_list = []\n",
        "    round_list = np.arange(0.5, 5.5, 0.5)\n",
        "\n",
        "    for value in predictions:\n",
        "        value_ori = value\n",
        "        compare_diff = 99999\n",
        "        value_round = 0\n",
        "\n",
        "        for rating in round_list:\n",
        "            compare_value = abs(value_ori - rating)\n",
        "\n",
        "            if compare_value < compare_diff: ## The absolute difference value that is closest to 0 is the rating the prediction will be rounded to\n",
        "                compare_diff = compare_value\n",
        "                value_round = rating\n",
        "\n",
        "        predictions_list.append(value_round)\n",
        "\n",
        "    prediction_dict = {'PREDICTION': predictions_list, 'TRUE': list(true)}\n",
        "    prediction_compare_df = pd.DataFrame(prediction_dict)\n",
        "\n",
        "    rating_accuracy = 0\n",
        "    like_dislike_tp = 0  ## \"Positive\" = Like\n",
        "    like_dislike_tn = 0  ## \"Negative\" = Dislike\n",
        "    like_dislike_fp = 0\n",
        "    like_dislike_fn = 0\n",
        "    prediction_length = len(prediction_compare_df)\n",
        "\n",
        "    ## Making the accuracy definition more flexible by covering a larger range:\n",
        "    rating_accuracy_flex = 0  ## If the prediction was within +/- 0.5 of the actual\n",
        "    like_dislike_tp_flex = 0  ## If the prediction was 3.5+ (instead of 4+), then it is a like\n",
        "    like_dislike_tn_flex = 0  ## If the prediction was 3.0-, then it is a dislike\n",
        "    like_dislike_fp_flex = 0\n",
        "    like_dislike_fn_flex = 0\n",
        "\n",
        "    progress_counter = 0\n",
        "\n",
        "    for index, row in prediction_compare_df.iterrows():\n",
        "        predict_like = 0\n",
        "        true_like = 0\n",
        "\n",
        "        if row.PREDICTION >= 4:\n",
        "            predict_like = 1\n",
        "\n",
        "        if row.TRUE >= 4:\n",
        "            true_like = 1\n",
        "\n",
        "        if row.PREDICTION == row.TRUE:  ## This is if the exact predicted rating value is the same as the actual value\n",
        "            rating_accuracy += 1\n",
        "\n",
        "        if predict_like == true_like:\n",
        "            if predict_like == 1:  ## Don't need to consider true_like to also be 1 since it is assumed it is with the nested if condition\n",
        "                like_dislike_tp += 1  ## True Positive\n",
        "\n",
        "            else:\n",
        "                like_dislike_tn += 1  ## True Negative\n",
        "\n",
        "        else:\n",
        "            if predict_like == 1:\n",
        "                like_dislike_fp += 1  ## False Positive\n",
        "\n",
        "            else:\n",
        "                like_dislike_fn += 1 ## False Negative\n",
        "\n",
        "        ####### FLEX starts:\n",
        "        predict_like_flex = 0\n",
        "        true_like_flex = 0\n",
        "\n",
        "        if row.PREDICTION >= 3.5:\n",
        "            predict_like_flex = 1\n",
        "\n",
        "        if row.TRUE >= 3.5:\n",
        "            true_like_flex = 1\n",
        "\n",
        "        if row.PREDICTION >= (row.TRUE - flex_range) and row.PREDICTION <= (row.TRUE + flex_range):  \n",
        "            rating_accuracy_flex += 1\n",
        "\n",
        "        if predict_like_flex == true_like_flex:\n",
        "            if predict_like_flex == 1:  \n",
        "                like_dislike_tp_flex += 1 \n",
        "\n",
        "            else:\n",
        "                like_dislike_tn_flex += 1 \n",
        "\n",
        "        else:\n",
        "            if predict_like_flex == 1:\n",
        "                like_dislike_fp_flex += 1 \n",
        "\n",
        "            else:\n",
        "                like_dislike_fn_flex += 1 \n",
        "\n",
        "        progress_counter += 1\n",
        "        if progress_counter % 100000 == 0:\n",
        "            print(str(progress_counter / prediction_length * 100) + '%')\n",
        "\n",
        "    rating_accuracy = rating_accuracy / prediction_length\n",
        "    like_dislike_accuracy = (like_dislike_tp + like_dislike_tn) / prediction_length\n",
        "\n",
        "    rating_accuracy_flex = rating_accuracy_flex / prediction_length\n",
        "    like_dislike_accuracy_flex = (like_dislike_tp_flex + like_dislike_tn_flex) / prediction_length\n",
        "\n",
        "    print('True Positive: {}, True Negative: {}, False Positive {}, False Negative {}'.format(like_dislike_tp, like_dislike_tn, like_dislike_fp, like_dislike_fn))\n",
        "    print('Rating Accuracy: {}, Catagorical Accuracy (Like/Dislike) {}'.format(rating_accuracy, like_dislike_accuracy))\n",
        "    print('------------------------------------------------------------------------------------------------------------')\n",
        "    print('FLEX True Positive: {}, FLEX True Negative: {}, FLEX False Positive {}, FLEX False Negative {}'.format(like_dislike_tp_flex, like_dislike_tn_flex, like_dislike_fp_flex, like_dislike_fn_flex))\n",
        "    print('FLEX Rating Accuracy: {}, FLEX Catagorical Accuracy (Like/Dislike) {}'.format(rating_accuracy_flex, like_dislike_accuracy_flex))\n",
        "    return"
      ],
      "metadata": {
        "id": "tfnLLq6KSHMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_shuffle_split(split=0.5):\n",
        "    movies_df_mod = pd.read_csv('data/movies_mod.csv')\n",
        "    ratings_df_removed = pd.read_csv('data/ratings_df_last_liked_movie_removed.csv')\n",
        "    \n",
        "    # Since ratings_df_removed is the template for merging, it will be shuffled:\n",
        "    ratings_df_removed = shuffle(ratings_df_removed)\n",
        "    \n",
        "    # Selecting a certain range from ratings_df_removed, train + test:\n",
        "    selection_range = int(len(ratings_df_removed) * (split))\n",
        "    ratings_df_removed = ratings_df_removed.iloc[: selection_range, :]\n",
        "    \n",
        "    # Merging begins:\n",
        "    ratings_df_removed = ratings_df_removed.merge(movies_df_mod, how= 'left', on= 'movieId').dropna()\n",
        "    del movies_df_mod\n",
        "\n",
        "\n",
        "    # Changing the columns names to differentiate between the columns of total_user_like_df and total_user_dislike_df:\n",
        "    total_user_like_df = pd.read_csv('data/total_user_like_df.csv')\n",
        "\n",
        "    like_columns = list(total_user_like_df.columns)\n",
        "    like_columns_modified = []\n",
        "\n",
        "    for column in like_columns:\n",
        "        if column == 'userId':\n",
        "            like_columns_modified.append('userId')\n",
        "        else:\n",
        "            modify_column = 'user_like_' + column\n",
        "            like_columns_modified.append(modify_column)\n",
        "\n",
        "    total_user_like_df.columns = like_columns_modified\n",
        "\n",
        "    ratings_df_removed = ratings_df_removed.merge(total_user_like_df, how= 'left', on= 'userId').dropna()\n",
        "    del total_user_like_df\n",
        "    \n",
        "\n",
        "    total_user_dislike_df = pd.read_csv('data/total_user_dislike_df.csv')    \n",
        "\n",
        "    dislike_columns = list(total_user_dislike_df.columns)\n",
        "    dislike_columns_modified = []\n",
        "\n",
        "    for column in dislike_columns:\n",
        "        if column == 'userId':\n",
        "            dislike_columns_modified.append('userId')\n",
        "        else:\n",
        "            modify_column = 'user_dislike_' + column\n",
        "            dislike_columns_modified.append(modify_column)\n",
        "\n",
        "    total_user_dislike_df.columns = dislike_columns_modified\n",
        "\n",
        "    # Merging all the DFs to create one final DF:\n",
        "    ratings_df_removed = ratings_df_removed.merge(total_user_dislike_df, how= 'left', on= 'userId').dropna()\n",
        "\n",
        "    # Removing loaded DFs to save on RAM space:\n",
        "    del total_user_dislike_df\n",
        "\n",
        "    movie_tags_df = pd.read_csv('data/final/movie_tags_df.csv')\n",
        "    ratings_df_removed = ratings_df_removed.merge(movie_tags_df, how= 'left', on= 'movieId').dropna()\n",
        "    del movie_tags_df\n",
        "\n",
        "    like_dislike_tags = (pd.read_csv('data/final/like_dislike_tags.csv')).astype('int64')\n",
        "    ratings_df_removed = ratings_df_removed.merge(like_dislike_tags, how= 'left', on= 'userId').dropna()\n",
        "    del like_dislike_tags\n",
        "    \n",
        "    like_columns_modified.remove('userId')\n",
        "    dislike_columns_modified.remove('userId')\n",
        "    like_columns.remove('userId')\n",
        "    \n",
        "    genres_like = ratings_df_removed.loc[:, like_columns_modified]\n",
        "    genres_dislike = ratings_df_removed.loc[:, dislike_columns_modified]\n",
        "    genres_movie = ratings_df_removed.loc[:, like_columns]\n",
        "    \n",
        "    # Generating the columns for the tag inputs for random forest:\n",
        "    rf_columns = []\n",
        "    for x in range(20): \n",
        "        rf_columns.append('LIKE_' + str(x))\n",
        "        rf_columns.append('DISLIKE_' + str(x))\n",
        "    for x in range(5):\n",
        "        rf_columns.append('TAG_' + str(x))\n",
        "        \n",
        "    rf_input = ratings_df_removed.loc[:, rf_columns]\n",
        "    \n",
        "    ratings = list(ratings_df_removed.rating)\n",
        "    \n",
        "    del ratings_df_removed\n",
        "    \n",
        "    return genres_like, genres_dislike, genres_movie, rf_input, ratings"
      ],
      "metadata": {
        "id": "dlXDDvnXSNCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Deep Learning/TensorFlow as the first model:\n",
        "## The goal of the model is to predict the rating the person would give to each movie\n",
        "### There will be three inputs: user liked genres, user disliked genres, and movie genres\n",
        "### The label will be the actual rating for the movie that the user gave it\n",
        "user_liked_genres = keras.Input(shape= (19,))\n",
        "user_disliked_genres = keras.Input(shape= (19,))\n",
        "movie_genres = keras.Input(shape= (19,))\n",
        "\n",
        "## Liked genres Input:\n",
        "liked_input = keras.layers.Dense(19, activation= 'relu')(user_liked_genres)\n",
        "liked_hidden_1 = keras.layers.Dense(50, activation= 'relu')(liked_input)\n",
        "liked_hidden_2 = keras.layers.Dense(50, activation= 'relu')(liked_hidden_1)\n",
        "\n",
        "## Disliked genres Input:\n",
        "disliked_input = keras.layers.Dense(19, activation= 'relu')(user_disliked_genres)\n",
        "disliked_hidden_1 = keras.layers.Dense(50, activation= 'relu')(disliked_input)\n",
        "disliked_hidden_2 = keras.layers.Dense(50, activation= 'relu')(disliked_hidden_1)\n",
        "\n",
        "## Movie genres Input:\n",
        "movie_input = keras.layers.Dense(19, activation= 'relu')(movie_genres)\n",
        "movie_hidden_1 = keras.layers.Dense(50, activation= 'relu')(movie_input)\n",
        "movie_hidden_2 = keras.layers.Dense(50, activation= 'relu')(movie_hidden_1)\n",
        "\n",
        "## Merging:\n",
        "merged_model = keras.layers.concatenate([liked_hidden_2, disliked_hidden_2, movie_hidden_2])\n",
        "merged_model_hidden_1 = keras.layers.Dense(150, activation= 'relu')(merged_model)\n",
        "merged_model_hidden_2 = keras.layers.Dense(75, activation= 'relu')(merged_model_hidden_1)\n",
        "merged_model_hidden_3 = keras.layers.Dense(50, activation= 'relu')(merged_model_hidden_2)\n",
        "\n",
        "## Output Layer:\n",
        "output_rating = keras.layers.Dense(1, activation= 'sigmoid')(merged_model_hidden_3)\n",
        "\n",
        "## Molding the Model togther:\n",
        "genres_model = keras.Model(inputs= [user_liked_genres, user_disliked_genres, movie_genres], outputs= output_rating)\n",
        "\n",
        "## Compiling the Model:\n",
        "genres_model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.001), loss= 'mean_squared_error')"
      ],
      "metadata": {
        "id": "CCGyb1SaSPN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models Training:\n",
        "if os.path.exists('models/') != True: \n",
        "    os.mkdir('models/')\n",
        "\n",
        "# Generating the datasets:\n",
        "genres_like, genres_dislike, genres_movie, rf_input, ratings = merge_shuffle_split() # Default split of the whole ratings.csv dataset is set to be 50%; already shuffled\n",
        "\n",
        "train_split = 0.5 ## This would be about 50% of original ratings.csv dataset\n",
        "split_index = int(len(ratings) * train_split)\n",
        "\n",
        "genres_like_train = genres_like.iloc[: split_index, :]\n",
        "genres_like_test = genres_like.iloc[split_index :, :]\n",
        "del genres_like ## Attempting to save RAM space\n",
        "\n",
        "genres_dislike_train = genres_dislike.iloc[: split_index, :]\n",
        "genres_dislike_test = genres_dislike.iloc[split_index :, :]\n",
        "del genres_dislike\n",
        "\n",
        "genres_movie_train = genres_movie.iloc[: split_index, :]\n",
        "genres_movie_test = genres_movie.iloc[split_index :, :]\n",
        "del genres_movie\n",
        "\n",
        "ratings_scaled = np.array(ratings) / 5\n",
        "ratings_scaled_train = ratings_scaled[: split_index]\n",
        "ratings_scaled_test = ratings_scaled[split_index :]\n",
        "\n",
        "batch_size = 500\n",
        "epochs = 10\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 5:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 0.001 * math.exp(0.1 * (5 - epoch))\n",
        "\n",
        "Learning_Rate_Callback = keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "class Save_Progress_Callback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None): ## Saving and printing after each epoch\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
        "        print(\"Epoch {}, loss is {:7.3f}, validation loss is {:7.3f}, learning rate is {}.\".format(epoch, logs[\"loss\"], logs[\"val_loss\"], lr))\n",
        "            \n",
        "### !!!!!!!!!!!!!! VERBOSE MUST BE SET TO 0 AS THE OUTPUT IS TOO LONG/LARGE AND WILL CRASH THE NOTEBOOK    \n",
        "genres_model.fit(x= [genres_like_train, genres_dislike_train, genres_movie_train], \n",
        "                  y= ratings_scaled_train, \n",
        "                  epochs= epochs, verbose= 0, batch_size= batch_size, validation_split= 0.1, shuffle= True,\n",
        "                  callbacks=[Learning_Rate_Callback, Save_Progress_Callback()])\n",
        "\n",
        "genres_model.save('models/genres_model.h5', overwrite= True, include_optimizer= True)\n",
        "\n",
        "# Random Forest:\n",
        "rf_input_train = rf_input.iloc[: split_index, :]\n",
        "rf_input_test = rf_input.iloc[split_index :, :]\n",
        "\n",
        "ratings_train = ratings[: split_index]\n",
        "ratings_test = ratings[split_index :]\n",
        "\n",
        "random_forest = RandomForestRegressor(n_estimators= 100, max_features= 'sqrt', verbose=2, random_state= True, n_jobs= -1) ## The number of trees is set to 100 due to high RAM usage\n",
        "random_forest.fit(rf_input_train, ratings_train)\n",
        "print(random_forest.score(rf_input_test, ratings_test))\n",
        "\n",
        "# Saving RF model:\n",
        "pickle.dump(random_forest, open('tags_model.sav', 'wb'))\n",
        "\n",
        "\n",
        "genres_model_predictions = (genres_model.predict(x= [genres_like_test, genres_dislike_test, genres_movie_test])) * 5 # Rescale back to original values\n",
        "random_forest_predict = random_forest.predict(rf_input_test)\n",
        "\n",
        "print('genres Model Stats:')\n",
        "stats(genres_model_predictions, ratings_scaled_test * 5)\n",
        "print('Tags Model Stats:')\n",
        "stats(random_forest_predict, ratings_test)\n",
        "\n",
        "# Creating a input for the combine_model:\n",
        "genres_model_predictions_list = []\n",
        "\n",
        "for prediction in genres_model_predictions:\n",
        "    genres_model_predictions_list.append(prediction[0])\n",
        "    \n",
        "merged_predictions = pd.DataFrame({'genres_model': genres_model_predictions_list, \n",
        "                                   'tag_model': list(random_forest_predict), \n",
        "                                   'genres_true': list(np.array(list(ratings_scaled_test)) * 5), \n",
        "                                   'tag_true': ratings_test}, \n",
        "                                  index= list(range(len(ratings_test))))\n",
        "\n",
        "# Using a linear regression for predictions adjustment:\n",
        "X = merged_predictions.loc[:, ['genres_model', 'tag_model']]\n",
        "y = np.array(merged_predictions.loc[:, 'genres_true']) \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25)\n",
        "\n",
        "line_reg = LinearRegression(n_jobs= -1).fit(X_train, y_train)\n",
        "print('Linear Regression R2:', line_reg.score(X_test, y_test))\n",
        "line_reg_predictions = line_reg.predict(X_test)\n",
        "\n",
        "# Saving linear regression model:\n",
        "pickle.dump(line_reg, open('combine_model.sav', 'wb'))\n",
        "\n",
        "# Rounding the predictions that are out of bounds:\n",
        "line_reg_predictions_rounded = []\n",
        "\n",
        "for prediction in line_reg_predictions:\n",
        "    rounded = prediction\n",
        "    if rounded > 5:\n",
        "        rounded = 5\n",
        "    elif rounded < 0.5:\n",
        "        rounded = 0.5\n",
        "    \n",
        "    line_reg_predictions_rounded.append(rounded)\n",
        "        \n",
        "\n",
        "stats(line_reg_predictions_rounded, y_test)"
      ],
      "metadata": {
        "id": "LN7q6Q0ySRjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "316de36a-5fea-416c-ed9e-8f863138910b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss is   0.043, validation loss is   0.041, learning rate is 0.0010000000474974513.\n",
            "Epoch 1, loss is   0.039, validation loss is   0.039, learning rate is 0.0010000000474974513.\n",
            "Epoch 2, loss is   0.038, validation loss is   0.038, learning rate is 0.0010000000474974513.\n",
            "Epoch 3, loss is   0.037, validation loss is   0.037, learning rate is 0.0010000000474974513.\n",
            "Epoch 4, loss is   0.037, validation loss is   0.037, learning rate is 0.0010000000474974513.\n",
            "Epoch 5, loss is   0.036, validation loss is   0.036, learning rate is 0.0010000000474974513.\n",
            "Epoch 6, loss is   0.035, validation loss is   0.036, learning rate is 0.0009048373904079199.\n",
            "Epoch 7, loss is   0.035, validation loss is   0.036, learning rate is 0.0008187307394109666.\n",
            "Epoch 8, loss is   0.034, validation loss is   0.035, learning rate is 0.0007408182136714458.\n",
            "Epoch 9, loss is   0.034, validation loss is   0.035, learning rate is 0.0006703200633637607.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building tree 1 of 100\n",
            "building tree 2 of 100\n",
            "building tree 3 of 100\n",
            "building tree 4 of 100\n",
            "building tree 5 of 100\n",
            "building tree 6 of 100\n",
            "building tree 7 of 100\n",
            "building tree 8 of 100\n",
            "building tree 9 of 100\n",
            "building tree 10 of 100\n",
            "building tree 11 of 100\n",
            "building tree 12 of 100\n",
            "building tree 13 of 100\n",
            "building tree 14 of 100\n",
            "building tree 15 of 100\n",
            "building tree 16 of 100\n",
            "building tree 17 of 100\n",
            "building tree 18 of 100\n",
            "building tree 19 of 100\n",
            "building tree 20 of 100\n",
            "building tree 21 of 100\n",
            "building tree 22 of 100\n",
            "building tree 23 of 100\n",
            "building tree 24 of 100\n",
            "building tree 25 of 100\n",
            "building tree 26 of 100\n",
            "building tree 27 of 100\n",
            "building tree 28 of 100\n",
            "building tree 29 of 100\n",
            "building tree 30 of 100\n",
            "building tree 31 of 100\n",
            "building tree 32 of 100\n",
            "building tree 33 of 100\n",
            "building tree 34 of 100\n",
            "building tree 35 of 100\n",
            "building tree 36 of 100\n",
            "building tree 37 of 100\n",
            "building tree 38 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   12.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building tree 39 of 100\n",
            "building tree 40 of 100\n",
            "building tree 41 of 100\n",
            "building tree 42 of 100\n",
            "building tree 43 of 100\n",
            "building tree 44 of 100\n",
            "building tree 45 of 100\n",
            "building tree 46 of 100\n",
            "building tree 47 of 100\n",
            "building tree 48 of 100\n",
            "building tree 49 of 100\n",
            "building tree 50 of 100\n",
            "building tree 51 of 100\n",
            "building tree 52 of 100\n",
            "building tree 53 of 100\n",
            "building tree 54 of 100\n",
            "building tree 55 of 100\n",
            "building tree 56 of 100\n",
            "building tree 57 of 100\n",
            "building tree 58 of 100\n",
            "building tree 59 of 100\n",
            "building tree 60 of 100\n",
            "building tree 61 of 100\n",
            "building tree 62 of 100\n",
            "building tree 63 of 100\n",
            "building tree 64 of 100\n",
            "building tree 65 of 100\n",
            "building tree 66 of 100\n",
            "building tree 67 of 100\n",
            "building tree 68 of 100\n",
            "building tree 69 of 100\n",
            "building tree 70 of 100\n",
            "building tree 71 of 100\n",
            "building tree 72 of 100\n",
            "building tree 73 of 100\n",
            "building tree 74 of 100\n",
            "building tree 75 of 100\n",
            "building tree 76 of 100\n",
            "building tree 77 of 100\n",
            "building tree 78 of 100\n",
            "building tree 79 of 100\n",
            "building tree 80 of 100\n",
            "building tree 81 of 100\n",
            "building tree 82 of 100\n",
            "building tree 83 of 100\n",
            "building tree 84 of 100\n",
            "building tree 85 of 100\n",
            "building tree 86 of 100\n",
            "building tree 87 of 100\n",
            "building tree 88 of 100\n",
            "building tree 89 of 100\n",
            "building tree 90 of 100\n",
            "building tree 91 of 100\n",
            "building tree 92 of 100\n",
            "building tree 93 of 100\n",
            "building tree 94 of 100\n",
            "building tree 95 of 100\n",
            "building tree 96 of 100\n",
            "building tree 97 of 100\n",
            "building tree 98 of 100\n",
            "building tree 99 of 100\n",
            "building tree 100 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   33.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    3.5s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12552934202118138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    3.5s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "genres Model Stats:\n",
            "83.85884878572386%\n",
            "True Positive: 37592, True Negative: 41610, False Positive 17073, False Negative 22973\n",
            "Rating Accuracy: 0.2369431772440628, Catagorical Accuracy (Like/Dislike) 0.6641788541526902\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "FLEX True Positive: 67126, FLEX True Negative: 15563, FLEX False Positive 28059, FLEX False Negative 8500\n",
            "FLEX Rating Accuracy: 0.6067690862739836, FLEX Catagorical Accuracy (Like/Dislike) 0.6934204347242721\n",
            "Tags Model Stats:\n",
            "83.85884878572386%\n",
            "True Positive: 29191, True Negative: 45441, False Positive 13242, False Negative 31374\n",
            "Rating Accuracy: 0.21560110022809606, Catagorical Accuracy (Like/Dislike) 0.6258553602576143\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "FLEX True Positive: 61245, FLEX True Negative: 17958, FLEX False Positive 25664, FLEX False Negative 14381\n",
            "FLEX Rating Accuracy: 0.5876576546357172, FLEX Catagorical Accuracy (Like/Dislike) 0.6641872400375688\n",
            "Linear Regression R2: 0.23832344373029146\n",
            "True Positive: 8204, True Negative: 11819, False Positive 2921, False Negative 6868\n",
            "Rating Accuracy: 0.2358446263249698, Catagorical Accuracy (Like/Dislike) 0.6716422916946196\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "FLEX True Positive: 16266, FLEX True Negative: 4701, FLEX False Positive 6314, FLEX False Negative 2531\n",
            "FLEX Rating Accuracy: 0.617469475379042, FLEX Catagorical Accuracy (Like/Dislike) 0.703307392996109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def top_10_recommendations(userId):\n",
        "    # Loading all the datasets needed:\n",
        "    movies_df_mod = pd.read_csv('data/movies_mod.csv')\n",
        "    ratings_df_removed = pd.read_csv('data/ratings_df_last_liked_movie_removed.csv')\n",
        "\n",
        "    \n",
        "    # Gathering all the movies in the dataset:\n",
        "    not_watched = list(movies_df_mod.movieId)\n",
        "    \n",
        "    # Selecting all movies that have not been seen by the user:\n",
        "    ratings_df_removed = ratings_df_removed[ratings_df_removed.userId == userId]\n",
        "    \n",
        "    if len(ratings_df_removed) ==  0:  ## First check for valid users/users with enough information \n",
        "        return print('User {} does not have enough information. 1'.format(userId))\n",
        "    \n",
        "    ratings_df_removed = ratings_df_removed.merge(movies_df_mod, how= 'left', on= 'movieId').dropna()\n",
        "    \n",
        "    if len(ratings_df_removed) ==  0:  ## Second check\n",
        "        return print('User {} does not have enough information. 2'.format(userId))\n",
        "    \n",
        "    watched = list(ratings_df_removed.movieId)\n",
        "    del ratings_df_removed  ## I find that not all variables are actually cleared in definitions; this is to ensure it removed from RAM\n",
        "    \n",
        "    # Finding the movies the user has not watched:\n",
        "    for movie in watched:\n",
        "        if movie in not_watched:\n",
        "            not_watched.remove(movie)\n",
        "            \n",
        "    # Loading in users' like and disliked genres:\n",
        "    total_user_like_df = pd.read_csv('data/total_user_like_df.csv')\n",
        "    total_user_dislike_df = pd.read_csv('data/total_user_dislike_df.csv') \n",
        "\n",
        "    \n",
        "    # Selecting from total_user_like_df and total_user_dislike_df to isolate only the userId input:\n",
        "    total_user_like_df = total_user_like_df[total_user_like_df.userId == userId]\n",
        "    \n",
        "    if len(total_user_like_df) ==  0:  ## Third check\n",
        "        return print('User {} does not have enough information. 3'.format(userId))\n",
        "    \n",
        "    total_user_dislike_df = total_user_dislike_df[total_user_dislike_df.userId == userId]\n",
        "    if len(total_user_dislike_df) ==  0:  ## Fourth check\n",
        "        return print('User {} does not have enough information. 4'.format(userId))\n",
        "            \n",
        "    # Changing the columns names to differentiate between the columns of total_user_like_df and total_user_dislike_df:\n",
        "\n",
        "    like_columns = list(total_user_like_df.columns)\n",
        "    like_columns_modified = []\n",
        "\n",
        "    for column in like_columns:\n",
        "        if column == 'userId':\n",
        "            like_columns_modified.append('userId')\n",
        "        else:\n",
        "            modify_column = 'user_like_' + column\n",
        "            like_columns_modified.append(modify_column)\n",
        "\n",
        "    total_user_like_df.columns = like_columns_modified\n",
        "    \n",
        "    dislike_columns = list(total_user_dislike_df.columns)\n",
        "    dislike_columns_modified = []\n",
        "\n",
        "    for column in dislike_columns:\n",
        "        if column == 'userId':\n",
        "            dislike_columns_modified.append('userId')\n",
        "        else:\n",
        "            modify_column = 'user_dislike_' + column\n",
        "            dislike_columns_modified.append(modify_column)\n",
        "\n",
        "    total_user_dislike_df.columns = dislike_columns_modified\n",
        "\n",
        "    # Loading in tags:\n",
        "    movie_tags_df = pd.read_csv('data/final/movie_tags_df.csv')\n",
        "    like_dislike_tags = (pd.read_csv('data/final/like_dislike_tags.csv')).astype('int64')\n",
        "    \n",
        "    # Selecting the movies that have not been seen from movie_tags_df and merging movies_df_mod and movie_tags_df:\n",
        "    template_df = pd.DataFrame({'movieId': not_watched}, index= list(range(len(not_watched)))) ## Creating a template DF for merging\n",
        "    template_df = template_df.merge(movies_df_mod, how= 'left', on= 'movieId').dropna()\n",
        "    template_df = template_df.merge(movie_tags_df, how= 'left', on= 'movieId').dropna()\n",
        "    del movie_tags_df\n",
        "    \n",
        "    # Selecting the user's tags:\n",
        "    like_dislike_tags = like_dislike_tags[like_dislike_tags.userId == userId]\n",
        "    if len(like_dislike_tags) ==  0:  ## Fifth check\n",
        "        return print('User {} does not have enough information. 5'.format(userId))\n",
        "\n",
        "    # Adding a userId column to the template DF so that merging is possible with total_user_like_df, total_user_dislike_df, and like_dislike_tags\n",
        "    template_df['userId'] = userId\n",
        "    template_df = template_df.merge(total_user_like_df, how= 'left', on= 'userId').dropna()\n",
        "    del total_user_like_df\n",
        "    template_df = template_df.merge(total_user_dislike_df, how= 'left', on= 'userId').dropna()\n",
        "    del total_user_dislike_df\n",
        "    template_df = template_df.merge(like_dislike_tags, how= 'left', on= 'userId').dropna()\n",
        "    del like_dislike_tags\n",
        "    \n",
        "    like_columns_modified.remove('userId')\n",
        "    dislike_columns_modified.remove('userId')\n",
        "    like_columns.remove('userId')\n",
        "\n",
        "    # Generating the columns for the tag inputs for random forest:\n",
        "    rf_columns = []\n",
        "    for x in range(20): \n",
        "        rf_columns.append('LIKE_' + str(x))\n",
        "        rf_columns.append('DISLIKE_' + str(x))\n",
        "    for x in range(5):\n",
        "        rf_columns.append('TAG_' + str(x))\n",
        "        \n",
        "    # Selecting out the inputs from the template DF by column names:\n",
        "    genres_like_input = template_df.loc[:, like_columns_modified]\n",
        "    genres_dislike_input = template_df.loc[:, dislike_columns_modified]\n",
        "    genres_movie_input = template_df.loc[:, like_columns]\n",
        "    \n",
        "    tags_input = template_df.loc[:, rf_columns]\n",
        "    \n",
        "    # Saving the movieId list:\n",
        "    movieId_list = list(template_df.movieId)\n",
        "    \n",
        "    del template_df\n",
        "    \n",
        "    # Loading in all models\n",
        "    genres_model = tf.keras.models.load_model('models/genres_model.h5', compile=True)\n",
        "    tags_model = pickle.load(open('tags_model.sav', 'rb'))\n",
        "    combine_model = pickle.load(open('combine_model.sav', 'rb'))\n",
        "    \n",
        "    # Predicting with the genres model and tags model:\n",
        "    genres_model_predictions = (genres_model.predict(x= [genres_like_input, genres_dislike_input, genres_movie_input])) * 5 ## Rescaling up; predicts a scaled and bound (sigmoid, 0-1) values\n",
        "    tags_model_predictions = tags_model.predict(tags_input)\n",
        "    \n",
        "    # Extracting and changing the Keras predictions into a 1-D format (list):\n",
        "    genres_model_predictions_list = []\n",
        "\n",
        "    for prediction in genres_model_predictions:\n",
        "        genres_model_predictions_list.append(prediction[0])\n",
        "    \n",
        "    # Using the predictions from the two models as the inputs for the combine_model:\n",
        "    combine_input = pd.DataFrame({'genres_predictions': genres_model_predictions_list, \n",
        "                                  'tags_predictions': tags_model_predictions}, \n",
        "                                 index= list(range(len(genres_model_predictions))))\n",
        "    \n",
        "    combine_model_predictions = combine_model.predict(combine_input)\n",
        "    \n",
        "    # Rounding the predictions that are out of bounds:\n",
        "    combine_model_predictions_rounded = []\n",
        "\n",
        "    for prediction in combine_model_predictions:\n",
        "        rounded = prediction\n",
        "        if rounded > 5:\n",
        "            rounded = 5\n",
        "        elif rounded < 0.5:\n",
        "            rounded = 0.5\n",
        "\n",
        "        combine_model_predictions_rounded.append(rounded)\n",
        "    \n",
        "    # Adding all predictions into one DF:\n",
        "    predictions_df = pd.DataFrame({'movieId': movieId_list,\n",
        "                                   'genres_predictions': genres_model_predictions_list, \n",
        "                                  'tags_predictions': tags_model_predictions,\n",
        "                                  'combine_predictions': combine_model_predictions_rounded}, \n",
        "                                 index= list(range(len(movieId_list))))\n",
        "    \n",
        "    # Sorting by combine_model_predictions_rounded and selecting the first 10 highest predicted ratings:\n",
        "    best_movies_df = predictions_df.sort_values(by= ['combine_predictions'], ascending=False).iloc[:10, :]\n",
        "    \n",
        "    # Finding adding the movie titles and information to highest 10:\n",
        "    best_movies_df = best_movies_df.merge(movies_df_mod, how= 'left', on= 'movieId').dropna()\n",
        "    del movies_df_mod\n",
        "    \n",
        "    return predictions_df, best_movies_df"
      ],
      "metadata": {
        "id": "0Pk1BW2tSeV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df, best_movies_df = top_10_recommendations(1) "
      ],
      "metadata": {
        "id": "tWv4lu9CSg5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bf7a76b-69b7-4c60-95e3-4a69e3849111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
            "Feature names unseen at fit time:\n",
            "- genres_predictions\n",
            "- tags_predictions\n",
            "Feature names seen at fit time, yet now missing:\n",
            "- genres_model\n",
            "- tag_model\n",
            "\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions_df)\n",
        "print(best_movies_df)"
      ],
      "metadata": {
        "id": "Ls73onYESiIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9750618-6e5f-4906-b7b7-b975c8404d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       movieId  genres_predictions  tags_predictions  combine_predictions\n",
            "0            1            4.268860             4.150             4.267282\n",
            "1            2            3.925070             3.595             3.796260\n",
            "2            3            3.781779             3.170             3.524909\n",
            "3            4            3.967724             3.345             3.731175\n",
            "4            5            3.556528             3.580             3.516000\n",
            "...        ...                 ...               ...                  ...\n",
            "11327   205287            3.585145             3.790             3.618664\n",
            "11328   205383            3.735043             3.790             3.730291\n",
            "11329   205425            3.556528             3.760             3.585731\n",
            "11330   206845            3.855473             3.575             3.736683\n",
            "11331   207309            3.795262             3.840             3.794506\n",
            "\n",
            "[11332 rows x 4 columns]\n",
            "   movieId  genres_predictions  tags_predictions  combine_predictions  \\\n",
            "0     1282            4.271165             4.325             4.336793   \n",
            "1     4886            4.268860             4.285             4.319581   \n",
            "2    86781            4.217859             4.335             4.300970   \n",
            "3    79091            4.455142             3.865             4.295596   \n",
            "4   141926            4.165092             4.410             4.290730   \n",
            "5    84637            4.268270             4.205             4.288150   \n",
            "6        1            4.268860             4.150             4.267282   \n",
            "7   106022            4.183357             4.265             4.248159   \n",
            "8     2080            4.242934             4.140             4.244101   \n",
            "9     1148            4.455142             3.730             4.243297   \n",
            "\n",
            "                                         title  \\\n",
            "0                              Fantasia (1940)   \n",
            "1                        Monsters, Inc. (2001)   \n",
            "2                             Incendies (2010)   \n",
            "3                         Despicable Me (2010)   \n",
            "4                    He Named Me Malala (2015)   \n",
            "5                       Gnomeo & Juliet (2011)   \n",
            "6                             Toy Story (1995)   \n",
            "7                   Toy Story of Terror (2013)   \n",
            "8                    Lady and the Tramp (1955)   \n",
            "9  Wallace & Gromit: The Wrong Trousers (1993)   \n",
            "\n",
            "                                              genres  YEAR  UPPER_STD  \\\n",
            "0                 Animation|Children|Fantasy|Musical  1940   4.878172   \n",
            "1        Adventure|Animation|Children|Comedy|Fantasy  2001   4.724483   \n",
            "2                                  Drama|Mystery|War  2010   4.724569   \n",
            "3                    Animation|Children|Comedy|Crime  2010   4.755651   \n",
            "4                                        Documentary  2015   3.500000   \n",
            "5  Adventure|Animation|Children|Comedy|Fantasy|Ro...  2011   3.851680   \n",
            "6        Adventure|Animation|Children|Comedy|Fantasy  1995   4.813895   \n",
            "7                          Animation|Children|Comedy  2013   4.353553   \n",
            "8                  Animation|Children|Comedy|Romance  1955   4.579965   \n",
            "9                    Animation|Children|Comedy|Crime  1993   5.000000   \n",
            "\n",
            "   LOWER_STD  AVG_RATING  ...  Musical  Action  Documentary  Adventure  \\\n",
            "0   2.561483    3.719828  ...        1       0            0          0   \n",
            "1   3.012516    3.868499  ...        0       0            0          1   \n",
            "2   3.275431    4.000000  ...        0       0            0          0   \n",
            "3   2.816513    3.786082  ...        0       0            0          0   \n",
            "4   1.000000    2.250000  ...        0       0            1          0   \n",
            "5   1.795379    2.823529  ...        0       0            0          1   \n",
            "6   2.982656    3.898276  ...        0       0            0          1   \n",
            "7   3.646447    4.000000  ...        0       0            0          0   \n",
            "8   2.716686    3.648325  ...        0       0            0          0   \n",
            "9   3.229959    4.148876  ...        0       0            0          0   \n",
            "\n",
            "   Children  Mystery  Romance  Drama  Animation  None  \n",
            "0         1        0        0      0          1     0  \n",
            "1         1        0        0      0          1     0  \n",
            "2         0        1        0      1          0     0  \n",
            "3         1        0        0      0          1     0  \n",
            "4         0        0        0      0          0     0  \n",
            "5         1        0        1      0          1     0  \n",
            "6         1        0        0      0          1     0  \n",
            "7         1        0        0      0          1     0  \n",
            "8         1        0        1      0          1     0  \n",
            "9         1        0        0      0          1     0  \n",
            "\n",
            "[10 rows x 30 columns]\n"
          ]
        }
      ]
    }
  ]
}